{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes based Spam Filter\n",
    "\n",
    "Study of probablity teaches us how to \n",
    "\n",
    "   - Assign probabilities to events based on certain conditions by using conditional probability rules.\n",
    "   - Assign probabilities to events based on whether they are in relationship of statistical independence or not with other events.\n",
    "   - Assign probabilities to events based on prior knowledge by using Bayes' theorem.\n",
    "   \n",
    "Using our knowledge of Navie Bayes theorem, we can build numerous practial applications. One such application is spam filter for incoming messages. Our aim is to build a new span filter which categorizes any incoming message as spam/non-spam. Since Naive Bayes relies on calculating probablity of an event based on prior knowledge, we need to build our system with dataset which contains messages that are already categorized as spam/non-spam by humans. \n",
    "\n",
    "To classify messages as spam or non-spam based on Naive Bayes algorithm, computer needs\n",
    "\n",
    "   - Learn how humans classify messages.\n",
    "   - Uses that human knowledge to estimate probabilities for new messages — probabilities for spam and non-spam.\n",
    "   - Classifies a new message based on these probability values — if the probability for spam is greater, then it classifies the message as spam. Otherwise, it classifies it as non-spam (if the two probability values are equal, then we may need a human to classify the message).\n",
    "\n",
    "So our first task is to \"teach\" the computer how to classify messages. To do that, we'll use the multinomial Naive Bayes algorithm along with a dataset of 5,572 SMS messages that are already classified by humans.\n",
    "\n",
    "The dataset was put together by Tiago A. Almeida and José María Gómez Hidalgo, and it can be downloaded from the The [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection). You can also download the dataset directly from [this link](https://dq-content.s3.amazonaws.com/433/SMSSpamCollection). The data collection process is described in more details on [this page](http://www.dt.fee.unicamp.br/~tiago/smsspamcollection/#composition), where you can also find some of the authors' papers.\n",
    "\n",
    "## Project goal\n",
    "\n",
    "Our goal is to write a program that classifies new messages with an accuracy greater than 80% — so we expect that more than 80% of the new messages will be classified correctly as spam or ham (non-spam)\n",
    "\n",
    "\n",
    "## Dataset exploration\n",
    "\n",
    "Let's start by reading in the dataset.Our data is stored in a tab separated file `SMSSpamCollection` and has no headers. We will read it and name the columns as `Label` and `SMS`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sms_spam = pd.read_csv('SMSSpamCollection', sep='\\t', header=None, names=['Label', 'SMS'])\n",
    "\n",
    "print(sms_spam.shape)\n",
    "sms_spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.593683\n",
       "spam    13.406317\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_spam[\"Label\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains training data which has nearly 87% non spam SMS and remaining 15% spam SMSes\n",
    "\n",
    "## Training set and Test set\n",
    "\n",
    "Once our spam filter is done, we'll need to test how good it is with classifying new messages. To test the spam filter, we're first going to split our dataset into two categories:\n",
    "\n",
    "   - A training set, which we'll use to \"train\" the computer how to classify messages.\n",
    "   - A test set, which we'll use to test how good the spam filter is with classifying new messages.\n",
    "\n",
    "Since we only have one dataset, we will partition into two separate datasets. One will serve as our training set and the second one as our test set. We're going to keep 80% of our dataset for training, and 20% for testing (we want to train the algorithm on as much data as possible, but we also want to have enough test data). The dataset has 5,572 messages, which means that:\n",
    "\n",
    "   - The training set will have 4,458 messages (about 80% of the dataset).\n",
    "   - The test set will have 1,114 messages (about 20% of the dataset).\n",
    "   \n",
    "Our test set will have 1114 messages which are already classified by a human. When the spam filter is ready, we're going to treat these messages as new and have the filter classify them. Once we have the results, we'll be able to compare the algorithm classification with that done by a human, and this way we'll see how good the spam filter really is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_training_sets(data,training_set_per = 0.8, \n",
    "                             test_setper = 0.2):\n",
    "    \"\"\"\n",
    "    this function will partition the dataset provided in first parameter\n",
    "    into two separate sets, one for training and another for test based on \n",
    "    the provided percentages\n",
    "    \n",
    "    Args:\n",
    "        data (pandas DataFrame)- input data set\n",
    "        training_set_per (float default 0.8 (80%)) = training set percentage\n",
    "        test_set_per (float default 0.2 (20%)) = test set percentage\n",
    "    Returns:\n",
    "        two separate dataframes divided based on training_set_per and test_setper\n",
    "        in that order\n",
    "    \"\"\"\n",
    "    \n",
    "    #randomize dataset\n",
    "    randomized_dataset = data.sample(frac=1, random_state= 1)\n",
    "    \n",
    "    training_index = round(len(randomized_dataset) * training_set_per)\n",
    "    training_set = randomized_dataset[:training_index].reset_index(drop=True)\n",
    "    test_set = randomized_dataset[training_index:].reset_index(drop=True)\n",
    "    return training_set, test_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4458, 2)\n",
      "(1114, 2)\n"
     ]
    }
   ],
   "source": [
    "training_set, test_set = split_test_training_sets(sms_spam)\n",
    "print(training_set.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets verify if the training and test sets have same proportion of spam/ham messages as in the original full data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.54105\n",
       "spam    13.45895\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set['Label'].value_counts(normalize=True) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.804309\n",
       "spam    13.195691\n",
       "Name: Label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['Label'].value_counts(normalize=True)* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are very close to the original dataset. The following table summarizes the percentages of ham/spam in training/test compared to original dataset\n",
    "\n",
    "|      | Orginal   | Training  | Test      |\n",
    "|------|-----------|-----------|-----------|\n",
    "| ham  | 86.593683 | 86.54105  | 86.804309 |\n",
    "| spam | 13.406317 | 13.45895  | 13.195691 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "We have created the datasets for training and testing. The training dataset will be used to train our application in classifying new messages as spam/ham. We will be using Naive Bayes algorithm, which dependens on calculating probability of each word being in spam/ham messages. The probability calculations is based on the word and its occurence count in a given messages. We will need to build  Vocabulary, and build a table which describes the word occurence count for every word in the sms\n",
    "\n",
    "**FROM**\n",
    "<img src=\"https://github.com/renuka28/datascience/blob/master/Building_a_Spam_Filter_with_Naive_Bayes/cpgp_dataset_1.png?raw=true\" alt=\"Original SMS messages\" title=\"Original SMS messages\" />\n",
    "\n",
    "**TO**\n",
    "<img src=\"https://github.com/renuka28/datascience/blob/master/Building_a_Spam_Filter_with_Naive_Bayes/cpgp_dataset_2.png?raw=true\" alt=\"new SMS table with word count\" title=\"new SMS table with word count\" />\n",
    "\n",
    "Some notes on the above transformation\n",
    "\n",
    "   - The SMS column doesn't exist anymore.\n",
    "   - Instead, the SMS column is replaced by a series of new columns, where each column represents a unique word from the vocabulary.\n",
    "   - Each row describes a single message. For instance, the first row corresponds to the message \"SECRET PRIZE! CLAIM SECRET PRIZE NOW!!\", and it has the values **spam, 2, 2, 1, 1, 0, 0, 0, 0, 0.** These values tell us that:\n",
    "       - The message is spam.\n",
    "       - The word \"secret\" occurs two times inside the message.\n",
    "       - The word \"prize\" occurs two times inside the message.\n",
    "       - The word \"claim\" occurs one time inside the message.\n",
    "       - The word \"now\" occurs one time inside the message.\n",
    "       - The words \"coming\", \"to\", \"my\", \"party\", and \"winner\" occur zero times inside the message.\n",
    "   - All words in the vocabulary are in lower case, so \"SECRET\" and \"secret\" come to be considered to be the same word.\n",
    "   - Punctuation is not taken into account anymore (for instance, we can't look at the table and conclude that the first message initially had three exclamation marks).\n",
    "   \n",
    " \n",
    "Let's begin the data cleaning process by removing the punctuation and bringing all the words to lower case.\n",
    "\n",
    "### Step 1 - Make words all lower case\n",
    "\n",
    "### Step 2 - Remove punctuations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before cleaning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yep, by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yes, princess. Are you going to make me moan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Havent.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I forgot 2 ask ü all smth.. There's a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       Yep, by the pretty sculpture\n",
       "1   ham      Yes, princess. Are you going to make me moan?\n",
       "2   ham                         Welp apparently he retired\n",
       "3   ham                                            Havent.\n",
       "4   ham  I forgot 2 ask ü all smth.. There's a card on ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Before cleaning\")\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_training_set(training_set):\n",
    "    training_set['SMS'] = training_set['SMS'].str.lower()\n",
    "    training_set['SMS'] = training_set['SMS'].str.replace('\\W', ' ')\n",
    "    return training_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>yep  by the pretty sculpture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>yes  princess  are you going to make me moan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>welp apparently he retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>havent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>i forgot 2 ask ü all smth   there s a card on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS\n",
       "0   ham                       yep  by the pretty sculpture\n",
       "1   ham      yes  princess  are you going to make me moan \n",
       "2   ham                         welp apparently he retired\n",
       "3   ham                                            havent \n",
       "4   ham  i forgot 2 ask ü all smth   there s a card on ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = clean_training_set(training_set)\n",
    "print (\"After cleaning\")\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Vocabulary\n",
    "\n",
    "Vocabulary for the training set is the complete list of all words found in our training set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_set):\n",
    "    training_set['SMS'] = training_set['SMS'].str.split()\n",
    "    vocabulary = []\n",
    "    for sms in training_set['SMS']:\n",
    "        for word in sms:\n",
    "            vocabulary.append(word)\n",
    "\n",
    "    vocabulary = list(set(vocabulary))\n",
    "    return training_set, vocabulary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length = 7783 \n",
      "\n",
      "SMS Word list \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                    [yep, by, the, pretty, sculpture]\n",
       "1    [yes, princess, are, you, going, to, make, me,...\n",
       "2                      [welp, apparently, he, retired]\n",
       "3                                             [havent]\n",
       "4    [i, forgot, 2, ask, ü, all, smth, there, s, a,...\n",
       "Name: SMS, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set, vocabulary = create_vocabulary(training_set)\n",
    "\n",
    "print(\"Vocabulary length = {} \\n\".format(len(vocabulary)))\n",
    "print(\"SMS Word list \")\n",
    "training_set['SMS'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final training set\n",
    "\n",
    "Now that we have our vocabulary, we can go ahead and do our data transformation. \n",
    "\n",
    "our target is to covert each SMS to a table of word counts which are stored in a table \n",
    "\n",
    "**FROM**\n",
    "<img src=\"https://github.com/renuka28/datascience/blob/master/Building_a_Spam_Filter_with_Naive_Bayes/cpgp_dataset_1.png?raw=true\" alt=\"Original SMS messages\" title=\"Original SMS messages\" />\n",
    "\n",
    "**TO**\n",
    "<img src=\"https://github.com/renuka28/datascience/blob/master/Building_a_Spam_Filter_with_Naive_Bayes/cpgp_dataset_2.png?raw=true\" alt=\"new SMS table with word count\" title=\"new SMS table with word count\" />\n",
    "\n",
    "\n",
    "the above table will get converted into following dictionary\n",
    "\n",
    "word_counts_per_sms = {'secret': [2,1,1],\n",
    "                       'prize': [2,0,1],\n",
    "                       'claim': [1,0,1],\n",
    "                       'now': [1,0,1],\n",
    "                       'coming': [0,1,0],\n",
    "                       'to': [0,1,0],\n",
    "                       'my': [0,1,0],\n",
    "                       'party': [0,1,0],\n",
    "                       'winner': [0,0,1]\n",
    "                      }\n",
    "\n",
    "The table will look like below\n",
    "\n",
    "| secret | prize | claim | now | coming | to | my | party | winner |   |\n",
    "|--------|-------|-------|-----|--------|----|----|-------|--------|---|\n",
    "| 0      | 2     | 2     | 1   | 1      | 0  | 0  | 0     | 0      | 0 |\n",
    "| 1      | 1     | 0     | 0   | 0      | 1  | 1  | 1     | 1      | 0 |\n",
    "| 2      | 1     | 1     | 1   | 1      | 0  | 0  | 0     | 0      | 1 |\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_training_set(training_set, vocabulary):\n",
    "    # lets start by creating an empty dictionary with each word in our vacabulary as\n",
    "    # key and 0s for each message. After that we will go through all sms and \n",
    "    #update the count for each word\n",
    "    word_counts_per_sms = {unique_word: [0] * len(training_set['SMS']) for unique_word in vocabulary}\n",
    "    for index, sms in enumerate(training_set['SMS']):\n",
    "        for word in sms:\n",
    "            word_counts_per_sms[word][index] += 1\n",
    "    #lets create pandas data frame with the our dictionary\n",
    "    \n",
    "    word_counts = pd.DataFrame(word_counts_per_sms)\n",
    "    \n",
    "    #lets concatinate orginal sms dataframe with the newly constructed one\n",
    "    training_set_clean = pd.concat([training_set, word_counts], axis=1)\n",
    "    return training_set_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_set_clean = create_final_training_set(training_set, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>0</th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>000pes</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>02</th>\n",
       "      <th>...</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>é</th>\n",
       "      <th>ú1</th>\n",
       "      <th>ü</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yep, by, the, pretty, sculpture]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[yes, princess, are, you, going, to, make, me,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>[welp, apparently, he, retired]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>[havent]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>[i, forgot, 2, ask, ü, all, smth, there, s, a,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS  0  00  000  \\\n",
       "0   ham                  [yep, by, the, pretty, sculpture]  0   0    0   \n",
       "1   ham  [yes, princess, are, you, going, to, make, me,...  0   0    0   \n",
       "2   ham                    [welp, apparently, he, retired]  0   0    0   \n",
       "3   ham                                           [havent]  0   0    0   \n",
       "4   ham  [i, forgot, 2, ask, ü, all, smth, there, s, a,...  0   0    0   \n",
       "\n",
       "   000pes  008704050406  0089  01223585334  02 ...  zindgi  zoe  zogtorius  \\\n",
       "0       0             0     0            0   0 ...       0    0          0   \n",
       "1       0             0     0            0   0 ...       0    0          0   \n",
       "2       0             0     0            0   0 ...       0    0          0   \n",
       "3       0             0     0            0   0 ...       0    0          0   \n",
       "4       0             0     0            0   0 ...       0    0          0   \n",
       "\n",
       "   zouk  zyada  é  ú1  ü  〨ud  鈥  \n",
       "0     0      0  0   0  0    0  0  \n",
       "1     0      0  0   0  0    0  0  \n",
       "2     0      0  0   0  0    0  0  \n",
       "3     0      0  0   0  0    0  0  \n",
       "4     0      0  0   0  2    0  0  \n",
       "\n",
       "[5 rows x 7785 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Constants First\n",
    "\n",
    "We're now done with cleaning the training set, and we can begin creating the spam filter. The Naive Bayes algorithm will need to answer these two probability questions to be able to classify new messages:\n",
    "\n",
    "$$\n",
    "P(Spam | w_1,w_2, ..., w_n) \\propto P(Spam) \\cdot \\prod_{i=1}^{n}P(w_i|Spam)\n",
    "$$$$\n",
    "P(Ham | w_1,w_2, ..., w_n) \\propto P(Ham) \\cdot \\prod_{i=1}^{n}P(w_i|Ham)\n",
    "$$\n",
    "Also, to calculate P(wi|Spam) and P(wi|Ham) inside the formulas above, we'll need to use these equations:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "Some of the terms in the four equations above will have the same value for every new message. We can calculate the value of these terms once and avoid doing the computations again when a new messages comes in. Below, we'll use our training set to calculate:\n",
    "\n",
    "   - P(Spam) and P(Ham)\n",
    "   - NSpam, NHam, NVocabulary\n",
    "We'll also use Laplace smoothing and set $\\alpha = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_spam = \"p_spam\"\n",
    "p_ham = \"p_ham\"\n",
    "n_spam = \"n_spam\"\n",
    "n_ham = \"n_ham\"\n",
    "n_vocabulary = \"n_vocabulary\"\n",
    "alpha = \"alpha\"\n",
    "p_ham_denominator = \"p_ham_denominator\"\n",
    "p_spam_denominator = \"p_spam_denominator\"\n",
    "spam_messages = \"spam_messages\"\n",
    "ham_messages = \"ham_messages\"\n",
    "\n",
    "def calculate_constants(training_set_clean):\n",
    "    constants = {}\n",
    "    # Isolating spam and ham messages first\n",
    "    constants[spam_messages] = training_set_clean[training_set_clean['Label'] == 'spam']\n",
    "    \n",
    "    constants[ham_messages] = training_set_clean[training_set_clean['Label'] == 'ham']\n",
    "\n",
    "    # P(Spam) and P(Ham)\n",
    "    constants[p_spam] = len(constants[spam_messages]) / len(training_set_clean)\n",
    "    constants[p_ham] = len(constants[ham_messages]) / len(training_set_clean)\n",
    "    print(\"p_spam = {} and p_ham = {}\".format(constants[p_spam],\n",
    "                                              constants[p_ham] ))\n",
    "\n",
    "\n",
    "    # N_Spam\n",
    "    n_words_per_spam_message = constants[spam_messages]['SMS'].apply(len)\n",
    "    constants[n_spam] = n_words_per_spam_message.sum()\n",
    "\n",
    "    # N_Ham\n",
    "    n_words_per_ham_message = constants[ham_messages]['SMS'].apply(len)\n",
    "    constants[n_ham] = n_words_per_ham_message.sum()\n",
    "\n",
    "    print(\"n_spam = {} and n_ham = {}\".format(constants[n_spam],\n",
    "                                              constants[n_ham] ))\n",
    "\n",
    "    # N_Vocabulary\n",
    "    constants[n_vocabulary] = len(vocabulary)\n",
    "    print(\"n_vocabulary = {}\".format(constants[n_vocabulary]))\n",
    "\n",
    "    # Laplace smoothing\n",
    "    constants[alpha] = 1\n",
    "    \n",
    "    #calculate denominators which will be constant\n",
    "    constants[p_ham_denominator] = constants[n_ham] +  (constants[alpha] * constants[n_vocabulary])\n",
    "    constants[p_spam_denominator] = constants[n_spam] +  (constants[alpha] * constants[n_vocabulary])\n",
    "    \n",
    "    return constants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_spam = 0.13458950201884254 and p_ham = 0.8654104979811574\n",
      "n_spam = 15190 and n_ham = 57237\n",
      "n_vocabulary = 7783\n",
      "(600, 7785)\n",
      "(3858, 7785)\n"
     ]
    }
   ],
   "source": [
    "constants = calculate_constants(training_set_clean)\n",
    "print(constants[spam_messages].shape)\n",
    "print(constants[ham_messages].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Parameters\n",
    "\n",
    "we managed to calculate a few terms for our equations:\n",
    "\n",
    "  - P(Spam) and P(Ham)\n",
    "  - NSpam, NHam, NVocabulary\n",
    "  \n",
    "As we've already mentioned, all these terms will have constant values in our equations for every new message (regardless of the message or each individual word in the message).\n",
    "\n",
    "However, P(Wi|Spam) and P(Wi|Ham) will vary depending on the individual words. For instance, P(\"secret\"|Spam) will have a certain probability value, while P(\"cousin\"|Spam) or P(\"lovely\"|Spam) will most likely have other values.\n",
    "\n",
    "Although both P(Wi|Spam) and P(Wi|Ham) vary depending on the word, the probability for each individual word is constant for every new message.\n",
    "\n",
    "For instance, let's say we receive two new messages:\n",
    "\n",
    "  - \"secret code\"\n",
    "  - \"secret party 2night\"\n",
    "We'll need to calculate P(\"secret\"|Spam) for both these messages, and we can use the training set to get the values we need to find a result for the equation below:\n",
    "\n",
    "The steps we take to calculate P(\"secret\"|Spam) will be identical for both of our new messages above, or for any other new message that contains the word \"secret\". The key detail here is that calculating P(\"secret\"|Spam) only depends on the training set, and as long as we don't make changes to the training set, P(\"secret\"|Spam) stays constant. The same reasoning also applies to P(\"secret\"|Ham).\n",
    "\n",
    "This means that we can use our training set to calculate the probability for each word in our vocabulary. If our vocabulary contained only the words \"lost\", \"navigate\", and \"sea\", then we'd need to calculate six probabilities:\n",
    "\n",
    "  - P(\"lost\"|Spam) and P(\"lost\"|Ham)\n",
    "  - P(\"navigate\"|Spam) and P(\"navigate\"|Ham)\n",
    "  - P(\"sea\"|Spam) and P(\"sea\"|Ham)\n",
    "\n",
    "We have 7,783 words in our vocabulary, which means we'll need to calculate a total of 15,566 probabilities. For each word, we need to calculate both P(Wi|Spam) and P(Wi|Ham).\n",
    "\n",
    "In more technical language, the probability values that P(wi|Spam) and P(wi|Ham) will take are called **`parameters`**.\n",
    "\n",
    "Now that we have the constant terms calculated above, we can move on with calculating the parameters $P(w_i|Spam)$ and $P(w_i|Ham)$. Each parameter will thus be a conditional probability value associated with each word in the vocabulary.\n",
    "\n",
    "The parameters are calculated using the formulas:\n",
    "\n",
    "$$\n",
    "P(w_i|Spam) = \\frac{N_{w_i|Spam} + \\alpha}{N_{Spam} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$$$\n",
    "P(w_i|Ham) = \\frac{N_{w_i|Ham} + \\alpha}{N_{Ham} + \\alpha \\cdot N_{Vocabulary}}\n",
    "$$\n",
    "\n",
    "\n",
    "The fact that we calculate so many values before even beginning the classification of new messages makes the Naive Bayes algorithm very fast (especially compared to other algorithms). When a new message comes in, most of the needed computations are already done, which enables the algorithm to almost instantly classify the new message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_parameters(constants, vocabulary):\n",
    "    parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
    "    parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
    "\n",
    "    # Calculate parameters\n",
    "    for word in vocabulary:\n",
    "        parameters_spam[word] = (constants[spam_messages][word].sum() + constants[alpha]) / constants[p_spam_denominator]\n",
    "        parameters_ham[word] = (constants[ham_messages][word].sum() + constants[alpha]) / constants[p_ham_denominator]\n",
    "    return parameters_ham, parameters_spam\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_ham, parameters_spam = calculate_parameters(constants, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7783"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(parameters_ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying A New Message\n",
    "\n",
    "ow that we've calculated all the constants and parameters we need, we can start creating the spam filter. The spam filter can be understood as a function that:\n",
    "\n",
    "  - Takes in as input a new message (w1, w2, ..., wn)\n",
    "  - Calculates P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn)\n",
    "  - Compares the values of P(Spam|w1, w2, ..., wn) and P(Ham|w1, w2, ..., wn), and:\n",
    "      - If P(Ham|w1, w2, ..., wn) > P(Spam|w1, w2, ..., wn), then the message is classified as ham.\n",
    "      - If P(Ham|w1, w2, ..., wn) < P(Spam|w1, w2, ..., wn), then the message is classified as spam.\n",
    "      - If P(Ham|w1, w2, ..., wn) = P(Spam|w1, w2, ..., wn), then the algorithm may request human help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message, parameters_spam, parameters_ham, constants):\n",
    "    import re\n",
    "    '''\n",
    "    message: a string\n",
    "    '''\n",
    "    # remove punctuations \n",
    "    message = re.sub('\\W', ' ', message)\n",
    "    message = message.lower().split()\n",
    "    \n",
    "    p_spam_given_message = constants[p_spam]\n",
    "    p_ham_given_message = constants[p_ham]\n",
    "\n",
    "    for word in message:\n",
    "        if word in parameters_spam:\n",
    "            p_spam_given_message *= parameters_spam[word]\n",
    "            \n",
    "        if word in parameters_ham:\n",
    "            p_ham_given_message *= parameters_ham[word]\n",
    "            \n",
    "#     print('P(Spam|message):', p_spam_given_message)\n",
    "#     print('P(Ham|message):', p_ham_given_message)\n",
    "    \n",
    "    if p_ham_given_message == p_spam_given_message:\n",
    "        ret_val = \"needs human intervention\"\n",
    "    else:\n",
    "        ret_val = \"ham\" if p_ham_given_message > p_spam_given_message else \"spam\"\n",
    "    return ret_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "print(classify('WINNER!! This is the secret code to unlock the money: C3421.', parameters_spam, parameters_ham, constants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"Sounds good, Tom, then see u there\", parameters_spam, parameters_ham, constants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam\n"
     ]
    }
   ],
   "source": [
    "print(classify(\"you won the prize\", parameters_spam, parameters_ham, constants))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring the Spam Filter's Accuracy\n",
    "\n",
    "Hurray!!! our spam filter needs to be working. Lets see how it performs against our test data set of 1114 messages. Note that, in training, our algorithm didn't see these 1,114 messages, so every message in the test set is practically new from the perspective of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_test_set(message):\n",
    "    return classify(message, parameters_spam, parameters_ham, constants)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>SMS</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Later i guess. I needa do mcat study too.</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>But i haf enuff space got like 4 mb...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 10 mths? Update to latest Oran...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>All sounds good. Fingers . Makes it difficult ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>All done, all handed in. Don't know if mega sh...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                                SMS predicted\n",
       "0   ham          Later i guess. I needa do mcat study too.       ham\n",
       "1   ham             But i haf enuff space got like 4 mb...       ham\n",
       "2  spam  Had your mobile 10 mths? Update to latest Oran...      spam\n",
       "3   ham  All sounds good. Fingers . Makes it difficult ...       ham\n",
       "4   ham  All done, all handed in. Don't know if mega sh...       ham"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set['predicted'] = test_set['SMS'].apply(classify_test_set)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll write a function to measure the accuracy of our spam filter to find out how well our spam filter does.\n",
    "\n",
    "We can compare the predicted values with the actual values to measure how good our spam filter is with classifying new messages. To make the measurement, we'll use accuracy as a metric:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1100\n",
      "Incorrect: 14\n",
      "Accuracy: 98.74%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = test_set.shape[0]\n",
    "    \n",
    "for row in test_set.iterrows():\n",
    "    row = row[1]\n",
    "    if row['Label'] == row['predicted']:\n",
    "        correct += 1\n",
    "        \n",
    "print('Correct:', correct)\n",
    "print('Incorrect:', total - correct)\n",
    "print('Accuracy: {0:.2f}%'.format(correct/total * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is close to 98.74%, which is really good. Our spam filter looked at 1,114 messages that it hasn't seen in training, and classified 1,100 correctly.\n",
    "\n",
    "\n",
    "Next Steps\n",
    "In this project, we managed to build a spam filter for SMS messages using the multinomial Naive Bayes algorithm. The filter had an accuracy of 98.74% on the test set we used, which is a pretty good result. Our initial goal was an accuracy of over 80%, and we managed to do way better than that.\n",
    "\n",
    "## Next steps include:\n",
    "\n",
    "   - Analyze the 14 messages that were classified incorrectly and try to figure out why the algorithm classified them incorrectly\n",
    "   - Make the filtering process more complex by making the algorithm sensitive to letter case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
